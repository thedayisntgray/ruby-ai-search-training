{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b62aab9-268b-4420-a7ba-1d9ca3f7aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/Lednik7/CLIP-ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acf4cce9-8a67-42bb-8c75-4bf3a753567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/Lednik7/CLIP-ONNX.git\n",
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "# !pip install onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81ef7f5-25e8-42b4-8ca1-01bccb1b462a",
   "metadata": {},
   "source": [
    "## Export CLIP embeddings to ONNX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed03027a-2148-4866-815a-680770549aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported CLIP image-encoder to clip_image_encoder.onnx\n"
     ]
    }
   ],
   "source": [
    "#image only\n",
    "# import torch\n",
    "# import clip\n",
    "# from PIL import Image\n",
    "\n",
    "# # 1) Load your CLIP model\n",
    "# device = \"cpu\"\n",
    "# clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "# clip_model.eval()\n",
    "\n",
    "# # 2) Define a wrapper module that returns image embeddings\n",
    "# class CLIPImageEncoder(torch.nn.Module):\n",
    "#     def __init__(self, clip_model):\n",
    "#         super().__init__()\n",
    "#         self.clip_model = clip_model\n",
    "\n",
    "#     def forward(self, image):\n",
    "#         # CLIP’s encode_image returns an unnormalized embedding of size [batch_size, 512]\n",
    "#         return self.clip_model.encode_image(image)\n",
    "\n",
    "# image_encoder = CLIPImageEncoder(clip_model).to(device)\n",
    "\n",
    "# # 3) Prepare a dummy input for shape tracing\n",
    "# dummy_image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)  # [1, 3, 224, 224]\n",
    "\n",
    "# # 4) Export the custom image-encoder to ONNX\n",
    "# torch.onnx.export(\n",
    "#     image_encoder,\n",
    "#     dummy_image,                       # Only one input\n",
    "#     \"clip_image_encoder.onnx\",\n",
    "#     export_params=True,\n",
    "#     opset_version=14,                 # for scaled_dot_product_attention\n",
    "#     do_constant_folding=True,\n",
    "#     input_names=[\"image_input\"],      # single input\n",
    "#     output_names=[\"image_features\"],  # single output\n",
    "#     dynamic_axes={\n",
    "#         \"image_input\": {0: \"batch_size\"},\n",
    "#         \"image_features\": {0: \"batch_size\"},\n",
    "#     },\n",
    "# )\n",
    "# print(\"Exported CLIP image-encoder to clip_image_encoder.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0aa0cc8-1184-4f69-ad60-4c21218af3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/opt/conda/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:5385: UserWarning: Exporting aten::index operator of advanced indexing in opset 14 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported CLIP image+text encoder to clip_image_text_encoder.onnx\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "# 1) Load your CLIP model\n",
    "device = \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "clip_model.eval()\n",
    "\n",
    "# 2) Create a wrapper that returns both image and text embeddings\n",
    "class CLIPEncoder(torch.nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.clip_model = clip_model\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        # image:  [batch_size_img, 3, 224, 224]\n",
    "        # text:   [batch_size_txt, 77]\n",
    "        image_embeds = self.clip_model.encode_image(image)   # shape [batch_size_img, 512]\n",
    "        text_embeds  = self.clip_model.encode_text(text)     # shape [batch_size_txt, 512]\n",
    "        return image_embeds, text_embeds\n",
    "\n",
    "# Instantiate our wrapper\n",
    "clip_encoder = CLIPEncoder(clip_model).to(device)\n",
    "clip_encoder.eval()\n",
    "\n",
    "# 3) Dummy inputs for shape tracing\n",
    "dummy_image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)  # shape [1, 3, 224, 224]\n",
    "dummy_text  = clip.tokenize([\"hello world\"]).to(device)                   # shape [1, 77]\n",
    "\n",
    "# 4) Export to ONNX with two inputs and two outputs\n",
    "input_names  = [\"image_input\", \"text_input\"]\n",
    "output_names = [\"image_features\", \"text_features\"]\n",
    "\n",
    "torch.onnx.export(\n",
    "    clip_encoder,\n",
    "    (dummy_image, dummy_text),       # pass both as a tuple\n",
    "    \"clip_image_text_encoder.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=14,                # needed for scaled_dot_product_attention\n",
    "    do_constant_folding=True,\n",
    "    input_names=input_names,\n",
    "    output_names=output_names,\n",
    "    dynamic_axes={\n",
    "        \"image_input\":     {0: \"batch_size_img\"},\n",
    "        \"text_input\":      {0: \"batch_size_txt\"},\n",
    "        \"image_features\":  {0: \"batch_size_img\"},\n",
    "        \"text_features\":   {0: \"batch_size_txt\"},\n",
    "    },\n",
    ")\n",
    "print(\"Exported CLIP image+text encoder to clip_image_text_encoder.onnx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9c2855-040a-404f-8039-cd05a8a9b721",
   "metadata": {},
   "source": [
    "# Text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5ccbe50-f8de-4a8f-857c-b822f0e79b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying another way as seen here: https://github.com/openai/CLIP/issues/122#issuecomment-1172928859"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9f7cdd6-0e9f-43da-bfa7-0a5d94d180b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/onnx/symbolic_opset9.py:5385: UserWarning: Exporting aten::index operator of advanced indexing in opset 14 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 36\u001b[0m\n\u001b[1;32m     11\u001b[0m m\u001b[38;5;241m.\u001b[39mforward(dummy_image, dummy_texts)  \u001b[38;5;66;03m# Original CLIP result (1)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mexport(m, (dummy_image, dummy_texts), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip_vit.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m, export_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m   input_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIMAGE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTEXT\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     15\u001b[0m   output_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOGITS_PER_IMAGE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOGITS_PER_TEXT\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m   }\n\u001b[1;32m     33\u001b[0m )\n\u001b[0;32m---> 36\u001b[0m \u001b[43mresult\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "# ! pip install onnx onnxruntime\n",
    "# This seems like it works\n",
    "import clip\n",
    "import torch\n",
    "\n",
    "# Change model to ViT-B/32\n",
    "m, pre = clip.load(\"ViT-B/32\")\n",
    "npx = m.visual.input_resolution  # Should be 224 for ViT-B/32\n",
    "dummy_image = torch.randn(10, 3, npx, npx)\n",
    "dummy_texts = clip.tokenize([\"quick brown fox\", \"lorem ipsum\"])\n",
    "m.forward(dummy_image, dummy_texts)  # Original CLIP result (1)\n",
    "\n",
    "torch.onnx.export(m, (dummy_image, dummy_texts), \"clip_vit.onnx\", export_params=True,\n",
    "  input_names=[\"IMAGE\", \"TEXT\"],\n",
    "  output_names=[\"LOGITS_PER_IMAGE\", \"LOGITS_PER_TEXT\"],\n",
    "  opset_version=14,\n",
    "  dynamic_axes={\n",
    "      \"IMAGE\": {\n",
    "          0: \"image_batch_size\",\n",
    "      },\n",
    "      \"TEXT\": {\n",
    "          0: \"text_batch_size\",\n",
    "      },\n",
    "      \"LOGITS_PER_IMAGE\": {\n",
    "          0: \"image_batch_size\",\n",
    "          1: \"text_batch_size\",\n",
    "      },\n",
    "      \"LOGITS_PER_TEXT\": {\n",
    "          0: \"text_batch_size\",\n",
    "          1: \"image_batch_size\",\n",
    "      },\n",
    "  }\n",
    ")\n",
    "\n",
    "\n",
    "# result  # verify that result is comparable to (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b3b5ef-0364-436b-960e-77cd59c733e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify with onnxruntime\n",
    "import onnxruntime as ort\n",
    "\n",
    "ort_sess = ort.InferenceSession(\"clip_vit.onnx\")\n",
    "result = ort_sess.run([\"LOGITS_PER_IMAGE\", \"LOGITS_PER_TEXT\"], \n",
    "  {\"IMAGE\": dummy_image.numpy(), \"TEXT\": dummy_texts.numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c8034d-9afc-4010-a3d5-836e93136aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Lednik7/CLIP-ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76833cf6-b306-4aac-8043-89bb4f2df75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-28 20:20:57--  https://github.com/openai/CLIP/blob/main/CLIP.png?raw=true\n",
      "Resolving github.com (github.com)... 140.82.113.3\n",
      "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github.com/openai/CLIP/raw/refs/heads/main/CLIP.png [following]\n",
      "--2024-12-28 20:20:57--  https://github.com/openai/CLIP/raw/refs/heads/main/CLIP.png\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/openai/CLIP/refs/heads/main/CLIP.png [following]\n",
      "--2024-12-28 20:20:57--  https://raw.githubusercontent.com/openai/CLIP/refs/heads/main/CLIP.png\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 416 Range Not Satisfiable\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -c -O CLIP.png https://github.com/openai/CLIP/blob/main/CLIP.png?raw=true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49ea935-2236-4e1f-b5bd-0f81d98053f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# onnx cannot work with cuda\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=\"cpu\", jit=False)\n",
    "\n",
    "# batch first\n",
    "image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).cpu() # [1, 3, 224, 224]\n",
    "image_onnx = image.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "# batch first\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).cpu() # [3, 77]\n",
    "text_onnx = text.detach().cpu().numpy().astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dda189-af20-48d8-9ef7-8e52c11ab8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# onnx cannot work with cuda\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=\"cpu\", jit=False)\n",
    "\n",
    "# batch first\n",
    "image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).cpu() # [1, 3, 224, 224]\n",
    "image_onnx = image.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "# batch first\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).cpu() # [3, 77]\n",
    "text_onnx = text.detach().cpu().numpy().astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328298d6-29c9-484e-b474-8db66fa8c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clip_onnx import clip_onnx\n",
    "\n",
    "visual_path = \"clip_visual.onnx\"\n",
    "textual_path = \"clip_textual.onnx\"\n",
    "\n",
    "onnx_model = clip_onnx(model, visual_path=visual_path, textual_path=textual_path)\n",
    "onnx_model.convert2onnx(image, text, verbose=True)\n",
    "# ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "onnx_model.start_sessions(providers=[\"CPUExecutionProvider\"]) # cpu mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9554f95-13fb-41d1-ab34-0f69c00dd633",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = onnx_model.encode_image(image_onnx)\n",
    "text_features = onnx_model.encode_text(text_onnx)\n",
    "\n",
    "logits_per_image, logits_per_text = onnx_model(image_onnx, text_onnx)\n",
    "probs = logits_per_image.softmax(dim=-1).detach().cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421067 0.00299571]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789ea93a-8eac-4f85-9787-bc6b37b8284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "802608cd-c7df-459f-997b-f302980023e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-28 20:19:37--  https://clip-as-service.s3.us-east-2.amazonaws.com/models/onnx/ViT-B-32/visual.onnx\n",
      "Resolving clip-as-service.s3.us-east-2.amazonaws.com (clip-as-service.s3.us-east-2.amazonaws.com)... 52.219.98.154, 3.5.132.132, 3.5.130.179, ...\n",
      "Connecting to clip-as-service.s3.us-east-2.amazonaws.com (clip-as-service.s3.us-east-2.amazonaws.com)|52.219.98.154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 351519609 (335M) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘visual.onnx’\n",
      "\n",
      "visual.onnx         100%[===================>] 335.23M  16.0MB/s    in 47s     \n",
      "\n",
      "2024-12-28 20:20:24 (7.17 MB/s) - ‘visual.onnx’ saved [351519609/351519609]\n",
      "\n",
      "--2024-12-28 20:20:24--  https://clip-as-service.s3.us-east-2.amazonaws.com/models/onnx/ViT-B-32/textual.onnx\n",
      "Resolving clip-as-service.s3.us-east-2.amazonaws.com (clip-as-service.s3.us-east-2.amazonaws.com)... 3.5.130.26, 52.219.143.10, 52.219.93.74, ...\n",
      "Connecting to clip-as-service.s3.us-east-2.amazonaws.com (clip-as-service.s3.us-east-2.amazonaws.com)|3.5.130.26|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 254120034 (242M) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘textual.onnx’\n",
      "\n",
      "textual.onnx        100%[===================>] 242.35M  19.5MB/s    in 12s     \n",
      "\n",
      "2024-12-28 20:20:36 (20.4 MB/s) - ‘textual.onnx’ saved [254120034/254120034]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://clip-as-service.s3.us-east-2.amazonaws.com/models/onnx/ViT-B-32/visual.onnx\n",
    "!wget https://clip-as-service.s3.us-east-2.amazonaws.com/models/onnx/ViT-B-32/textual.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b40eeb2b-818d-40e5-9fa2-022526ef4351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/Lednik7/CLIP-ONNX.git\n",
      "  Cloning https://github.com/Lednik7/CLIP-ONNX.git to /tmp/pip-req-build-hcczvaoc\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/Lednik7/CLIP-ONNX.git /tmp/pip-req-build-hcczvaoc\n",
      "  Resolved https://github.com/Lednik7/CLIP-ONNX.git to commit ebd4852b7d3ebf116709abf33b26832acaba947b\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch==1.13.1\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-zbwz92uf\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-zbwz92uf\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (21.3)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.64.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.5.1)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.20.1)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->clip==1.0) (3.0.9)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.12.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.13.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.16.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (11.2.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (12.3.1.170)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (12.4.127)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (2.8.7)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (2.21.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy==1.13.1->torch->clip==1.0) (1.2.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.23.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (9.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.1)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'clip_onnx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mclip_onnx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clip_onnx\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'clip_onnx'"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/Lednik7/CLIP-ONNX.git\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from clip_onnx import clip_onnx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a7b73ac-7fac-49ec-b435-9e5a4c22f083",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clip_onnx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m onnx_model \u001b[38;5;241m=\u001b[39m \u001b[43mclip_onnx\u001b[49m(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m onnx_model\u001b[38;5;241m.\u001b[39mload_onnx(visual_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisual.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m                      textual_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtextual.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m                      logit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100.0000\u001b[39m) \u001b[38;5;66;03m# model.logit_scale.exp()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m onnx_model\u001b[38;5;241m.\u001b[39mstart_sessions(providers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPUExecutionProvider\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clip_onnx' is not defined"
     ]
    }
   ],
   "source": [
    "onnx_model = clip_onnx(None)\n",
    "onnx_model.load_onnx(visual_path=\"visual.onnx\",\n",
    "                     textual_path=\"textual.onnx\",\n",
    "                     logit_scale=100.0000) # model.logit_scale.exp()\n",
    "onnx_model.start_sessions(providers=[\"CPUExecutionProvider\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1313a36c-fa52-4082-a07d-59cbbe1581a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
