{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbaa182-cf64-420c-ba8f-6353a4fc9e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b57050-ec61-439f-9fa0-cb4ef5a4a0cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c6c07ed-cceb-4f33-842e-7ec22e6ff0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5249 images\n",
      "Processing 100 images...\n",
      "Processing batch 1 of 4\n",
      "Batch embeddings type: Numo::DFloat\n",
      "Batch embeddings shape: [32, 512]\n",
      "Processing batch 2 of 4\n",
      "Batch embeddings type: Numo::DFloat\n",
      "Batch embeddings shape: [32, 512]\n",
      "Processing batch 3 of 4\n",
      "Batch embeddings type: Numo::DFloat\n",
      "Batch embeddings shape: [32, 512]\n",
      "Processing batch 4 of 4\n",
      "Batch embeddings type: Numo::DFloat\n",
      "Batch embeddings shape: [4, 512]\n",
      "Final embeddings shape: [100, 512]\n"
     ]
    }
   ],
   "source": [
    "require 'onnxruntime'\n",
    "require 'rmagick'\n",
    "require 'torch-rb'\n",
    "require 'numo/narray'\n",
    "\n",
    "def preprocess_image(image)\n",
    "  # Resize to 224x224 using RMagick\n",
    "  image = image.resize_to_fill(224, 224)\n",
    "  \n",
    "  # Convert to RGB array and normalize\n",
    "  rgb_data = image.export_pixels(0, 0, image.columns, image.rows, 'RGB')\n",
    "  rgb_array = Numo::DFloat.cast(rgb_data).reshape(3, 224, 224) / 255.0\n",
    "  \n",
    "  # Apply CLIP normalization\n",
    "  means = Numo::DFloat[0.48145466, 0.4578275, 0.40821073]\n",
    "  stds = Numo::DFloat[0.26862954, 0.26130258, 0.27577711]\n",
    "  \n",
    "  3.times do |c|\n",
    "    rgb_array[c, true, true] -= means[c]\n",
    "    rgb_array[c, true, true] /= stds[c]\n",
    "  end\n",
    "  \n",
    "  # Add batch dimension\n",
    "  rgb_array.reshape(1, 3, 224, 224)\n",
    "end\n",
    "\n",
    "def simple_tokenize(text, context_length=77)\n",
    "  # Simple tokenizer that creates zero array\n",
    "  Numo::Int64.zeros(1, context_length)\n",
    "end\n",
    "\n",
    "# def get_image_embeddings(model, image_paths, batch_size: 32)\n",
    "#   embeddings_list = []\n",
    "  \n",
    "#   (0...image_paths.size).step(batch_size) do |start_idx|\n",
    "#     batch_paths = image_paths[start_idx, batch_size]\n",
    "    \n",
    "#     # Process batch of images\n",
    "#     batch_tensors = batch_paths.map do |path|\n",
    "#       image = Magick::Image.read(path).first\n",
    "#       preprocess_image(image)\n",
    "#     end\n",
    "    \n",
    "#     # Combine batch\n",
    "#     batch_tensor = Numo::DFloat.zeros(batch_paths.size, 3, 224, 224)\n",
    "#     batch_tensors.each_with_index do |tensor, i|\n",
    "#       batch_tensor[i, true, true, true] = tensor[0, true, true, true]\n",
    "#     end\n",
    "    \n",
    "#     # Create dummy text tokens\n",
    "#     dummy_texts = Numo::Int64.zeros(batch_paths.size, 77)\n",
    "    \n",
    "#     # Run inference\n",
    "#     outputs = model.predict({\n",
    "#       \"image_input\" => batch_tensor,\n",
    "#       \"text_input\" => dummy_texts\n",
    "#     })\n",
    "    \n",
    "#     # Get embeddings - ensure it's a Numo::NArray\n",
    "#     batch_embeddings = Numo::NArray.cast(outputs[\"image_features\"])\n",
    "    \n",
    "#     # Add debugging\n",
    "#     puts \"Batch embeddings type: #{batch_embeddings.class}\"\n",
    "#     puts \"Batch embeddings shape: #{batch_embeddings.shape}\"\n",
    "    \n",
    "#     # Convert to torch tensor\n",
    "#     begin\n",
    "#       batch_embeddings_torch = Torch.tensor(batch_embeddings.to_a)\n",
    "#       norms = batch_embeddings_torch.norm(2, 1, keepdim: true)\n",
    "#       normalized_embeddings = batch_embeddings_torch / norms\n",
    "      \n",
    "#       embeddings_list << normalized_embeddings\n",
    "#     rescue => e\n",
    "#       puts \"Error in conversion: #{e.message}\"\n",
    "#       puts \"Shape before conversion: #{batch_embeddings.shape}\"\n",
    "#       next\n",
    "#     end\n",
    "#   end\n",
    "  \n",
    "#   # Combine all batches\n",
    "#   Torch.cat(embeddings_list, dim: 0)\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Usage\n",
    "# begin\n",
    "#   # Load ONNX model\n",
    "#   model = OnnxRuntime::Model.new('clip_image_text_encoder.onnx')\n",
    "  \n",
    "#   # Process directory of images\n",
    "#   image_dir = 'house_data_png'\n",
    "#   image_paths = Dir[File.join(image_dir, '*')]\n",
    "#   puts \"Found #{image_paths.length} images\"\n",
    "  \n",
    "#   # Get embeddings\n",
    "#   embeddings = get_image_embeddings(model, image_paths)\n",
    "#   puts \"Final embeddings shape: #{embeddings.shape}\"\n",
    "  \n",
    "# rescue => e\n",
    "#   puts \"Error: #{e.message}\"\n",
    "#   puts e.backtrace\n",
    "# end\n",
    "\n",
    "\n",
    "def get_image_embeddings(model, image_paths, batch_size: 32, limit: nil)\n",
    " # Apply limit if specified\n",
    " image_paths = image_paths[0...limit] if limit\n",
    " \n",
    " puts \"Processing #{image_paths.length} images...\"\n",
    " \n",
    " embeddings_list = []\n",
    " \n",
    " (0...image_paths.size).step(batch_size) do |start_idx|\n",
    "   batch_paths = image_paths[start_idx, batch_size]\n",
    "   \n",
    "   puts \"Processing batch #{start_idx/batch_size + 1} of #{(image_paths.size.to_f/batch_size).ceil}\"\n",
    "   \n",
    "   # Process batch of images\n",
    "   batch_tensors = batch_paths.map do |path|\n",
    "     image = Magick::Image.read(path).first\n",
    "     preprocess_image(image)\n",
    "   end\n",
    "   \n",
    "   # Combine batch\n",
    "   batch_tensor = Numo::DFloat.zeros(batch_paths.size, 3, 224, 224)\n",
    "   batch_tensors.each_with_index do |tensor, i|\n",
    "     batch_tensor[i, true, true, true] = tensor[0, true, true, true]\n",
    "   end\n",
    "   \n",
    "   # Create dummy text tokens\n",
    "   dummy_texts = Numo::Int64.zeros(batch_paths.size, 77)\n",
    "   \n",
    "   # Run inference\n",
    "   outputs = model.predict({\n",
    "     \"image_input\" => batch_tensor,\n",
    "     \"text_input\" => dummy_texts\n",
    "   })\n",
    "   \n",
    "   # Get embeddings - ensure it's a Numo::NArray\n",
    "   batch_embeddings = Numo::NArray.cast(outputs[\"image_features\"])\n",
    "   \n",
    "   # Add debugging\n",
    "   puts \"Batch embeddings type: #{batch_embeddings.class}\"\n",
    "   puts \"Batch embeddings shape: #{batch_embeddings.shape}\"\n",
    "   \n",
    "   # Convert to torch tensor\n",
    "   begin\n",
    "     batch_embeddings_torch = Torch.tensor(batch_embeddings.to_a)\n",
    "     norms = batch_embeddings_torch.norm(2, 1, keepdim: true)\n",
    "     normalized_embeddings = batch_embeddings_torch / norms\n",
    "     \n",
    "     embeddings_list << normalized_embeddings\n",
    "   rescue => e\n",
    "     puts \"Error in conversion: #{e.message}\"\n",
    "     puts \"Shape before conversion: #{batch_embeddings.shape}\"\n",
    "     next\n",
    "   end\n",
    " end\n",
    " \n",
    " # Combine all batches\n",
    " Torch.cat(embeddings_list, dim: 0)\n",
    "end\n",
    "\n",
    "# Usage\n",
    "begin\n",
    " # Load ONNX model\n",
    " model = OnnxRuntime::Model.new('clip_image_text_encoder.onnx')\n",
    " \n",
    " # Process directory of images\n",
    " image_dir = 'house_data_png'\n",
    " image_paths = Dir[File.join(image_dir, '*')]\n",
    " puts \"Found #{image_paths.length} images\"\n",
    " \n",
    " # Get embeddings for first 100 images\n",
    " embeddings = get_image_embeddings(model, image_paths, limit: 100)\n",
    " puts \"Final embeddings shape: #{embeddings.shape}\"\n",
    " \n",
    "rescue => e\n",
    " puts \"Error: #{e.message}\"\n",
    " puts e.backtrace\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72504666-3cb9-4772-9b01-c52eba8c90e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0086, -0.0172,  0.0142,  ...,  0.0412, -0.0064,  0.0027],\n",
       "        [ 0.0140,  0.0025,  0.0138,  ...,  0.0485,  0.0043,  0.0073],\n",
       "        [-0.0073, -0.0085,  0.0317,  ...,  0.0320, -0.0004,  0.0363],\n",
       "        ...,\n",
       "        [ 0.0066, -0.0006,  0.0121,  ...,  0.0584, -0.0078, -0.0052],\n",
       "        [ 0.0128, -0.0024,  0.0054,  ...,  0.0549, -0.0059, -0.0072],\n",
       "        [-0.0009, -0.0002,  0.0173,  ...,  0.0421,  0.0040,  0.0087]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1524c568-decf-4924-ba12-d271ca4ee4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 100 images...\n",
      "Processing batch 1 of 4\n",
      "Batch embeddings type: Numo::DFloat\n",
      "Batch embeddings shape: [32, 512]\n",
      "Processing batch 2 of 4\n",
      "Batch embeddings type: Numo::DFloat\n",
      "Batch embeddings shape: [32, 512]\n",
      "Processing batch 3 of 4\n",
      "Batch embeddings type: Numo::DFloat\n",
      "Batch embeddings shape: [32, 512]\n",
      "Processing batch 4 of 4\n",
      "Batch embeddings type: Numo::DFloat\n",
      "Batch embeddings shape: [4, 512]\n",
      "\n",
      "Top 5 matches for query: 'large kitchen island colonial'\n",
      "1. bath_114.png (similarity: 0.241)\n",
      "2. bath_1150.png (similarity: 0.24)\n",
      "3. bath_1013.png (similarity: 0.239)\n",
      "4. bath_1169.png (similarity: 0.238)\n",
      "5. bath_1059.png (similarity: 0.237)\n",
      "\n",
      "Top 5 matches for query: 'white marble shower stall'\n",
      "1. bath_114.png (similarity: 0.241)\n",
      "2. bath_1150.png (similarity: 0.24)\n",
      "3. bath_1013.png (similarity: 0.239)\n",
      "4. bath_1169.png (similarity: 0.238)\n",
      "5. bath_1059.png (similarity: 0.237)\n",
      "\n",
      "Top 5 similar images to bath_1006.png:\n",
      "1. bath_1111.png (similarity: 0.987)\n",
      "2. bath_1149.png (similarity: 0.985)\n",
      "3. bath_1141.png (similarity: 0.984)\n",
      "4. bath_1136.png (similarity: 0.981)\n",
      "5. bath_1163.png (similarity: 0.98)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[\"house_data_png/bath_1111.png\", \"house_data_png/bath_1149.png\", \"house_data_png/bath_1141.png\", \"house_data_png/bath_1136.png\", \"house_data_png/bath_1163.png\"], [0.9867925643920898, 0.9853135347366333, 0.9835642576217651, 0.9810572862625122, 0.980038046836853]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to find nearest neighbors\n",
    "def find_nearest_neighbors(embeddings, query_index, k=5)\n",
    "  # Get the query embedding\n",
    "  query_embedding = embeddings[query_index]\n",
    "  \n",
    "  # Calculate similarities with all embeddings\n",
    "  query_embedding = query_embedding.reshape(1, -1)  # Make it 2D: [1, embed_dim]\n",
    "  \n",
    "  # Calculate cosine similarity\n",
    "  similarities = Torch.matmul(embeddings, query_embedding.t())\n",
    "  \n",
    "  # Get top k (excluding the first which is the query itself)\n",
    "  values, indices = similarities.flatten.topk(k + 1)\n",
    "  \n",
    "  # Convert to Ruby arrays and remove the self-similarity\n",
    "  similarity_scores = values.to_a[1..-1]  # Skip first one (self)\n",
    "  neighbor_indices = indices.to_a[1..-1]  # Skip first one (self)\n",
    "  \n",
    "  return neighbor_indices, similarity_scores\n",
    "end\n",
    "\n",
    "# Save embeddings to file\n",
    "def save_embeddings(embeddings, filename)\n",
    "  File.open(filename, 'wb') do |file|\n",
    "    Marshal.dump(embeddings.to_a, file)\n",
    "  end\n",
    "end\n",
    "\n",
    "# Load embeddings from file\n",
    "def load_embeddings(filename)\n",
    "  embeddings_array = Marshal.load(File.read(filename))\n",
    "  Torch.tensor(embeddings_array)\n",
    "end\n",
    "\n",
    "# Encode text to the same vector space as images\n",
    "def embed_text(model, text)\n",
    "  # Create simple tokenizer (77 is CLIP's context length)\n",
    "  dummy_tokens = Numo::Int64.zeros(1, 77)\n",
    "  \n",
    "  # Get text embeddings\n",
    "  outputs = model.predict({\n",
    "    \"image_input\" => Numo::DFloat.zeros(1, 3, 224, 224),  # dummy image\n",
    "    \"text_input\" => dummy_tokens\n",
    "  })\n",
    "  \n",
    "  # Get and normalize text embeddings\n",
    "  text_embedding = Torch.tensor(outputs[\"text_features\"].to_a)\n",
    "  text_embedding /= text_embedding.norm(2, -1, keepdim: true)\n",
    "  text_embedding\n",
    "end\n",
    "\n",
    "# Search by text query\n",
    "def search_by_text(model, embeddings, image_paths, query, k=5)\n",
    "  # Get text embedding\n",
    "  text_embedding = embed_text(model, query)\n",
    "  \n",
    "  # Calculate similarities\n",
    "  similarities = Torch.matmul(embeddings, text_embedding.t())\n",
    "  \n",
    "  # Get top k results\n",
    "  values, indices = similarities.flatten.topk(k)\n",
    "  \n",
    "  # Convert to Ruby arrays\n",
    "  similarity_scores = values.to_a\n",
    "  result_indices = indices.to_a\n",
    "  \n",
    "  # Get matching image paths\n",
    "  result_paths = result_indices.map { |idx| image_paths[idx] }\n",
    "  \n",
    "  puts \"\\nTop #{k} matches for query: '#{query}'\"\n",
    "  result_paths.zip(similarity_scores).each_with_index do |(path, score), i|\n",
    "    puts \"#{i+1}. #{File.basename(path)} (similarity: #{score.round(3)})\"\n",
    "  end\n",
    "  \n",
    "  return result_paths, similarity_scores\n",
    "end\n",
    "\n",
    "# Search by image index\n",
    "def search_by_image(embeddings, image_paths, index, k=5)\n",
    "  neighbor_indices, similarity_scores = find_nearest_neighbors(embeddings, index, k)\n",
    "  \n",
    "  result_paths = neighbor_indices.map { |idx| image_paths[idx] }\n",
    "  \n",
    "  puts \"\\nTop #{k} similar images to #{File.basename(image_paths[index])}:\"\n",
    "  result_paths.zip(similarity_scores).each_with_index do |(path, score), i|\n",
    "    puts \"#{i+1}. #{File.basename(path)} (similarity: #{score.round(3)})\"\n",
    "  end\n",
    "  \n",
    "  return result_paths, similarity_scores\n",
    "end\n",
    "\n",
    "# Usage example:\n",
    "begin\n",
    "  # Load model and embeddings\n",
    "  model = OnnxRuntime::Model.new('clip_image_text_encoder.onnx')\n",
    "  \n",
    "  # Either generate embeddings:\n",
    "  image_paths = Dir[File.join('house_data_png', '*')]\n",
    "  embeddings = get_image_embeddings(model, image_paths, limit: 100)\n",
    "  \n",
    "  # Or load pre-saved embeddings:\n",
    "  # embeddings = load_embeddings('house_data_png.marshal')\n",
    "  \n",
    "  # Example searches\n",
    "  search_by_text(model, embeddings, image_paths, \"large kitchen island colonial\")\n",
    "  search_by_text(model, embeddings, image_paths, \"white marble shower stall\")\n",
    "  \n",
    "  # Search by image example\n",
    "  search_by_image(embeddings, image_paths, 5)  # Changed from 505 to 5 since we limited to 100\n",
    "  \n",
    "rescue => e\n",
    "  puts \"Error: #{e.message}\"\n",
    "  puts e.backtrace\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72dd5b6b-7917-46e6-944d-07aaaea89841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 100 images...\n",
      "Processing batch 1 of 4\n",
      "Batch embeddings type: Numo::DFloat\n",
      "Batch embeddings shape: [32, 512]\n",
      "Processing batch 2 of 4\n",
      "Batch embeddings type: Numo::DFloat\n",
      "Batch embeddings shape: [32, 512]\n",
      "Processing batch 3 of 4\n",
      "Batch embeddings type: Numo::DFloat\n",
      "Batch embeddings shape: [32, 512]\n",
      "Processing batch 4 of 4\n",
      "Batch embeddings type: Numo::DFloat\n",
      "Batch embeddings shape: [4, 512]\n",
      "Searching for: 'large kitchen island colonial'\n",
      "\n",
      "Top 5 matches for query: 'large kitchen island colonial'\n",
      "1. bath_114.png (similarity: 0.241)\n",
      "Error: undefined method `format=' for #<Object:0x00007ffffaf83f98>\n",
      "Did you mean?  format\n",
      "(irb):14:in `block in display_image'\n",
      "(irb):14:in `to_blob'\n",
      "(irb):14:in `display_image'\n",
      "(irb):34:in `block in search_by_text'\n",
      "(irb):32:in `each'\n",
      "(irb):32:in `each_with_index'\n",
      "(irb):32:in `search_by_text'\n",
      "(irb):67:in `<top (required)>'\n",
      "/home/jovyan/.local/share/gem/ruby/3.1.0/gems/irb-1.6.2/lib/irb/workspace.rb:119:in `eval'\n",
      "/home/jovyan/.local/share/gem/ruby/3.1.0/gems/irb-1.6.2/lib/irb/workspace.rb:119:in `evaluate'\n",
      "/home/jovyan/.local/share/gem/ruby/3.1.0/gems/irb-1.6.2/lib/irb/context.rb:502:in `evaluate'\n",
      "/home/jovyan/.local/share/gem/ruby/3.1.0/gems/iruby-0.7.4/lib/iruby/backend.rb:55:in `eval'\n",
      "/home/jovyan/.local/share/gem/ruby/3.1.0/gems/iruby-0.7.4/lib/iruby/backend.rb:11:in `eval'\n",
      "/home/jovyan/.local/share/gem/ruby/3.1.0/gems/iruby-0.7.4/lib/iruby/kernel.rb:203:in `execute_request'\n",
      "/home/jovyan/.local/share/gem/ruby/3.1.0/gems/iruby-0.7.4/lib/iruby/kernel.rb:139:in `dispatch'\n",
      "/home/jovyan/.local/share/gem/ruby/3.1.0/gems/iruby-0.7.4/lib/iruby/kernel.rb:127:in `run'\n",
      "/home/jovyan/.local/share/gem/ruby/3.1.0/gems/iruby-0.7.4/lib/iruby/command.rb:106:in `run_kernel'\n",
      "/home/jovyan/.local/share/gem/ruby/3.1.0/gems/iruby-0.7.4/lib/iruby/command.rb:37:in `run'\n",
      "/home/jovyan/.local/share/gem/ruby/3.1.0/gems/iruby-0.7.4/bin/iruby:5:in `<top (required)>'\n",
      "/home/jovyan/.local/share/gem/ruby/3.1.0/bin/iruby:25:in `load'\n",
      "/home/jovyan/.local/share/gem/ruby/3.1.0/bin/iruby:25:in `<main>'\n"
     ]
    }
   ],
   "source": [
    "require 'rmagick'\n",
    "require 'base64'\n",
    "\n",
    "# Function to display image in Jupyter notebook\n",
    "def display_image(image_path)\n",
    "  # Read image using RMagick\n",
    "  img = Magick::Image.read(image_path).first\n",
    "  \n",
    "  # Resize if too large (optional)\n",
    "  if img.columns > 500 || img.rows > 500\n",
    "    img = img.resize_to_fit(500, 500)\n",
    "  end\n",
    "  \n",
    "  # Convert to blob and base64\n",
    "  blob = img.to_blob { self.format = 'JPEG' }\n",
    "  base64_img = Base64.encode64(blob)\n",
    "  \n",
    "  # Display using IRuby\n",
    "  IRuby.display IRuby.html \"<img src='data:image/jpeg;base64,#{base64_img}'>\"\n",
    "end\n",
    "\n",
    "# Modified search functions to display images\n",
    "def search_by_text(model, embeddings, image_paths, query, k=5)\n",
    "  text_embedding = embed_text(model, query)\n",
    "  similarities = Torch.matmul(embeddings, text_embedding.t())\n",
    "  values, indices = similarities.flatten.topk(k)\n",
    "  \n",
    "  similarity_scores = values.to_a\n",
    "  result_indices = indices.to_a\n",
    "  result_paths = result_indices.map { |idx| image_paths[idx] }\n",
    "  \n",
    "  puts \"\\nTop #{k} matches for query: '#{query}'\"\n",
    "  result_paths.zip(similarity_scores).each_with_index do |(path, score), i|\n",
    "    puts \"#{i+1}. #{File.basename(path)} (similarity: #{score.round(3)})\"\n",
    "    display_image(path)\n",
    "  end\n",
    "  \n",
    "  return result_paths, similarity_scores\n",
    "end\n",
    "\n",
    "def search_by_image(embeddings, image_paths, index, k=5)\n",
    "  puts \"\\nQuery image:\"\n",
    "  display_image(image_paths[index])\n",
    "  \n",
    "  neighbor_indices, similarity_scores = find_nearest_neighbors(embeddings, index, k)\n",
    "  result_paths = neighbor_indices.map { |idx| image_paths[idx] }\n",
    "  \n",
    "  puts \"\\nTop #{k} similar images to #{File.basename(image_paths[index])}:\"\n",
    "  result_paths.zip(similarity_scores).each_with_index do |(path, score), i|\n",
    "    puts \"#{i+1}. #{File.basename(path)} (similarity: #{score.round(3)})\"\n",
    "    display_image(path)\n",
    "  end\n",
    "  \n",
    "  return result_paths, similarity_scores\n",
    "end\n",
    "\n",
    "# Usage example:\n",
    "begin\n",
    "  # Load model and embeddings\n",
    "  model = OnnxRuntime::Model.new('clip_image_text_encoder.onnx')\n",
    "  \n",
    "  # Generate or load embeddings\n",
    "  image_paths = Dir[File.join('house_data_png', '*')]\n",
    "  embeddings = get_image_embeddings(model, image_paths, limit: 100)\n",
    "  \n",
    "  # Example searches with image display\n",
    "  puts \"Searching for: 'large kitchen island colonial'\"\n",
    "  search_by_text(model, embeddings, image_paths, \"large kitchen island colonial\")\n",
    "  \n",
    "  puts \"\\nSearching for: 'white marble shower stall'\"\n",
    "  search_by_text(model, embeddings, image_paths, \"white marble shower stall\")\n",
    "  \n",
    "  puts \"\\nSearching by image index 5:\"\n",
    "  search_by_image(embeddings, image_paths, 5)\n",
    "  \n",
    "rescue => e\n",
    "  puts \"Error: #{e.message}\"\n",
    "  puts e.backtrace\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e49c6-69d0-43dc-a2b8-6eb8a6706047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43df625a-0655-43ee-b718-1119ffcb0ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f12378e-23f0-48df-839a-a7319b2c291b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d821ba40-6b15-4025-a695-3c082cb1cbd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff91f0d-5039-481a-94ba-d12b69c2e413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ed6f7-6d66-4ebc-a85b-870bdff6d521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b974acb-0649-4cb8-bb7f-2cc27d7c6ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 3.1.3",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "3.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
