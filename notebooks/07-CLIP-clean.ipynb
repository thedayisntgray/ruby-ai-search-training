{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbaa182-cf64-420c-ba8f-6353a4fc9e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b57050-ec61-439f-9fa0-cb4ef5a4a0cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c6c07ed-cceb-4f33-842e-7ec22e6ff0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5249 images\n",
      "Processing 100 images...\n",
      "Processing batch 1 of 4\n",
      "Batch embeddings type: Numo::DFloat\n",
      "Batch embeddings shape: [32, 512]\n",
      "Processing batch 2 of 4\n",
      "Batch embeddings type: Numo::DFloat\n",
      "Batch embeddings shape: [32, 512]\n",
      "Processing batch 3 of 4\n",
      "Batch embeddings type: Numo::DFloat\n",
      "Batch embeddings shape: [32, 512]\n",
      "Processing batch 4 of 4\n",
      "Batch embeddings type: Numo::DFloat\n",
      "Batch embeddings shape: [4, 512]\n",
      "Final embeddings shape: [100, 512]\n"
     ]
    }
   ],
   "source": [
    "require 'onnxruntime'\n",
    "require 'rmagick'\n",
    "require 'torch-rb'\n",
    "require 'numo/narray'\n",
    "\n",
    "def preprocess_image(image)\n",
    "  # Resize to 224x224 using RMagick\n",
    "  image = image.resize_to_fill(224, 224)\n",
    "  \n",
    "  # Convert to RGB array and normalize\n",
    "  rgb_data = image.export_pixels(0, 0, image.columns, image.rows, 'RGB')\n",
    "  rgb_array = Numo::DFloat.cast(rgb_data).reshape(3, 224, 224) / 255.0\n",
    "  \n",
    "  # Apply CLIP normalization\n",
    "  means = Numo::DFloat[0.48145466, 0.4578275, 0.40821073]\n",
    "  stds = Numo::DFloat[0.26862954, 0.26130258, 0.27577711]\n",
    "  \n",
    "  3.times do |c|\n",
    "    rgb_array[c, true, true] -= means[c]\n",
    "    rgb_array[c, true, true] /= stds[c]\n",
    "  end\n",
    "  \n",
    "  # Add batch dimension\n",
    "  rgb_array.reshape(1, 3, 224, 224)\n",
    "end\n",
    "\n",
    "def simple_tokenize(text, context_length=77)\n",
    "  # Simple tokenizer that creates zero array\n",
    "  Numo::Int64.zeros(1, context_length)\n",
    "end\n",
    "\n",
    "# def get_image_embeddings(model, image_paths, batch_size: 32)\n",
    "#   embeddings_list = []\n",
    "  \n",
    "#   (0...image_paths.size).step(batch_size) do |start_idx|\n",
    "#     batch_paths = image_paths[start_idx, batch_size]\n",
    "    \n",
    "#     # Process batch of images\n",
    "#     batch_tensors = batch_paths.map do |path|\n",
    "#       image = Magick::Image.read(path).first\n",
    "#       preprocess_image(image)\n",
    "#     end\n",
    "    \n",
    "#     # Combine batch\n",
    "#     batch_tensor = Numo::DFloat.zeros(batch_paths.size, 3, 224, 224)\n",
    "#     batch_tensors.each_with_index do |tensor, i|\n",
    "#       batch_tensor[i, true, true, true] = tensor[0, true, true, true]\n",
    "#     end\n",
    "    \n",
    "#     # Create dummy text tokens\n",
    "#     dummy_texts = Numo::Int64.zeros(batch_paths.size, 77)\n",
    "    \n",
    "#     # Run inference\n",
    "#     outputs = model.predict({\n",
    "#       \"image_input\" => batch_tensor,\n",
    "#       \"text_input\" => dummy_texts\n",
    "#     })\n",
    "    \n",
    "#     # Get embeddings - ensure it's a Numo::NArray\n",
    "#     batch_embeddings = Numo::NArray.cast(outputs[\"image_features\"])\n",
    "    \n",
    "#     # Add debugging\n",
    "#     puts \"Batch embeddings type: #{batch_embeddings.class}\"\n",
    "#     puts \"Batch embeddings shape: #{batch_embeddings.shape}\"\n",
    "    \n",
    "#     # Convert to torch tensor\n",
    "#     begin\n",
    "#       batch_embeddings_torch = Torch.tensor(batch_embeddings.to_a)\n",
    "#       norms = batch_embeddings_torch.norm(2, 1, keepdim: true)\n",
    "#       normalized_embeddings = batch_embeddings_torch / norms\n",
    "      \n",
    "#       embeddings_list << normalized_embeddings\n",
    "#     rescue => e\n",
    "#       puts \"Error in conversion: #{e.message}\"\n",
    "#       puts \"Shape before conversion: #{batch_embeddings.shape}\"\n",
    "#       next\n",
    "#     end\n",
    "#   end\n",
    "  \n",
    "#   # Combine all batches\n",
    "#   Torch.cat(embeddings_list, dim: 0)\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Usage\n",
    "# begin\n",
    "#   # Load ONNX model\n",
    "#   model = OnnxRuntime::Model.new('clip_image_text_encoder.onnx')\n",
    "  \n",
    "#   # Process directory of images\n",
    "#   image_dir = 'house_data_png'\n",
    "#   image_paths = Dir[File.join(image_dir, '*')]\n",
    "#   puts \"Found #{image_paths.length} images\"\n",
    "  \n",
    "#   # Get embeddings\n",
    "#   embeddings = get_image_embeddings(model, image_paths)\n",
    "#   puts \"Final embeddings shape: #{embeddings.shape}\"\n",
    "  \n",
    "# rescue => e\n",
    "#   puts \"Error: #{e.message}\"\n",
    "#   puts e.backtrace\n",
    "# end\n",
    "\n",
    "\n",
    "def get_image_embeddings(model, image_paths, batch_size: 32, limit: nil)\n",
    " # Apply limit if specified\n",
    " image_paths = image_paths[0...limit] if limit\n",
    " \n",
    " puts \"Processing #{image_paths.length} images...\"\n",
    " \n",
    " embeddings_list = []\n",
    " \n",
    " (0...image_paths.size).step(batch_size) do |start_idx|\n",
    "   batch_paths = image_paths[start_idx, batch_size]\n",
    "   \n",
    "   puts \"Processing batch #{start_idx/batch_size + 1} of #{(image_paths.size.to_f/batch_size).ceil}\"\n",
    "   \n",
    "   # Process batch of images\n",
    "   batch_tensors = batch_paths.map do |path|\n",
    "     image = Magick::Image.read(path).first\n",
    "     preprocess_image(image)\n",
    "   end\n",
    "   \n",
    "   # Combine batch\n",
    "   batch_tensor = Numo::DFloat.zeros(batch_paths.size, 3, 224, 224)\n",
    "   batch_tensors.each_with_index do |tensor, i|\n",
    "     batch_tensor[i, true, true, true] = tensor[0, true, true, true]\n",
    "   end\n",
    "   \n",
    "   # Create dummy text tokens\n",
    "   dummy_texts = Numo::Int64.zeros(batch_paths.size, 77)\n",
    "   \n",
    "   # Run inference\n",
    "   outputs = model.predict({\n",
    "     \"image_input\" => batch_tensor,\n",
    "     \"text_input\" => dummy_texts\n",
    "   })\n",
    "   \n",
    "   # Get embeddings - ensure it's a Numo::NArray\n",
    "   batch_embeddings = Numo::NArray.cast(outputs[\"image_features\"])\n",
    "   \n",
    "   # Add debugging\n",
    "   puts \"Batch embeddings type: #{batch_embeddings.class}\"\n",
    "   puts \"Batch embeddings shape: #{batch_embeddings.shape}\"\n",
    "   \n",
    "   # Convert to torch tensor\n",
    "   begin\n",
    "     batch_embeddings_torch = Torch.tensor(batch_embeddings.to_a)\n",
    "     norms = batch_embeddings_torch.norm(2, 1, keepdim: true)\n",
    "     normalized_embeddings = batch_embeddings_torch / norms\n",
    "     \n",
    "     embeddings_list << normalized_embeddings\n",
    "   rescue => e\n",
    "     puts \"Error in conversion: #{e.message}\"\n",
    "     puts \"Shape before conversion: #{batch_embeddings.shape}\"\n",
    "     next\n",
    "   end\n",
    " end\n",
    " \n",
    " # Combine all batches\n",
    " Torch.cat(embeddings_list, dim: 0)\n",
    "end\n",
    "\n",
    "# Usage\n",
    "begin\n",
    " # Load ONNX model\n",
    " model = OnnxRuntime::Model.new('clip_image_text_encoder.onnx')\n",
    " \n",
    " # Process directory of images\n",
    " image_dir = 'house_data_png'\n",
    " image_paths = Dir[File.join(image_dir, '*')]\n",
    " puts \"Found #{image_paths.length} images\"\n",
    " \n",
    " # Get embeddings for first 100 images\n",
    " embeddings = get_image_embeddings(model, image_paths, limit: 100)\n",
    " puts \"Final embeddings shape: #{embeddings.shape}\"\n",
    " \n",
    "rescue => e\n",
    " puts \"Error: #{e.message}\"\n",
    " puts e.backtrace\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72504666-3cb9-4772-9b01-c52eba8c90e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0086, -0.0172,  0.0142,  ...,  0.0412, -0.0064,  0.0027],\n",
       "        [ 0.0140,  0.0025,  0.0138,  ...,  0.0485,  0.0043,  0.0073],\n",
       "        [-0.0073, -0.0085,  0.0317,  ...,  0.0320, -0.0004,  0.0363],\n",
       "        ...,\n",
       "        [ 0.0066, -0.0006,  0.0121,  ...,  0.0584, -0.0078, -0.0052],\n",
       "        [ 0.0128, -0.0024,  0.0054,  ...,  0.0549, -0.0059, -0.0072],\n",
       "        [-0.0009, -0.0002,  0.0173,  ...,  0.0421,  0.0040,  0.0087]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570154c7-b373-48dd-9697-47cb7f61d2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 3.1.3",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "3.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
