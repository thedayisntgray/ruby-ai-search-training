{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84a25145-4603-48e6-94a9-0b1539d3d990",
   "metadata": {},
   "source": [
    "# Lab 7 - Multimodal search with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eaaaf1-dccf-44fe-a681-9b49588cba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "require 'httparty'\n",
    "require 'zip'\n",
    "require 'mini_magick'\n",
    "require 'torch-rb'\n",
    "require 'open_clip'\n",
    "require 'nearest_neighbors'\n",
    "require 'tqdm'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e1fa3d-cbaa-4d94-bda5-4d287dc46809",
   "metadata": {},
   "source": [
    "## Get the image dataset (interiors of houses)\n",
    "\n",
    "- Source: https://www.kaggle.com/datasets/mikhailma/house-rooms-streets-image-dataset/data\n",
    "- Cached: https://max.io/house_data_png.zip (resized to 256x256 and converted to PNG)\n",
    "- License: CC-0 Public Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ec361-c278-4cb5-9c82-878c92668260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download and extract the zip file\n",
    "def download_and_extract_zip(url, extract_to: '.')\n",
    "    puts \"Downloading and extracting #{url}\"\n",
    "    response = HTTParty.get(url)\n",
    "    Zip::File.open_buffer(response.body) do |zip_file|\n",
    "        zip_file.each do |entry|\n",
    "            entry.extract(File.join(extract_to, entry.name))\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# Download and extract the example images\n",
    "url = \"https://max.io/house_data_png.zip\"\n",
    "download_and_extract_zip(url)\n",
    "image_dir = 'house_data_png'\n",
    "image_paths = Dir[File.join(image_dir, '*')]\n",
    "puts \"Extracted #{image_paths.length} images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6185f03-fc2c-4aec-ba99-eb78d43f8d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our model\n",
    "model, transform, preprocess = OpenClip.create_model_and_transforms('ViT-B-32', pretrained: 'openai')\n",
    "\n",
    "# Print the model architecture\n",
    "model.eval\n",
    "puts model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dedfd7-0d7a-4cd2-8521-3efd05aa57b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infers images in batches\n",
    "def get_image_embeddings(image_paths, batch_size: 32)\n",
    "    embeddings = []\n",
    "    device = Torch.cuda.available? ? 'cuda' : 'cpu'\n",
    "\n",
    "    # Process images in batches\n",
    "    (0...image_paths.length).step(batch_size).each do |i|\n",
    "        batch_paths = image_paths[i...([i + batch_size, image_paths.length].min)]\n",
    "        batch_images = batch_paths.map do |path|\n",
    "            img = MiniMagick::Image.open(path)\n",
    "            transform.call(img).unsqueeze(0)\n",
    "        end\n",
    "\n",
    "        # Stack and process the batch\n",
    "        batch_images_tensor = Torch.vstack(batch_images).to(device)\n",
    "        \n",
    "        Torch.no_grad do\n",
    "            batch_embeddings = model.encode_image(batch_images_tensor)\n",
    "            embeddings.push(batch_embeddings)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Concatenate all embeddings\n",
    "    Torch.vstack(embeddings)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f86c3df-6a9b-4313-8fd6-202a141aeea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings = get_image_embeddings(image_paths, batch_size: 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23e59c9-ae46-41cb-aa96-133019d20502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization is required!\n",
    "image_embeddings /= image_embeddings.norm(dim: -1, keepdim: true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dd0ce7-58ce-437b-ac85-b0b431e34154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings\n",
    "File.open('house_data_png.marshal', 'wb') do |file|\n",
    "    Marshal.dump(image_embeddings.cpu.numpy, file)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36083c88-f25a-4178-a932-b2d5804a469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "puts \"#{image_embeddings.length} #{image_embeddings[0].shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b35845-38ce-49fe-a285-cdd6f81ef81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodes the text to the same vector space as the images\n",
    "def embed_text(text)\n",
    "    tokens = OpenClip::Tokenizer.tokenize([text])\n",
    "    Torch.no_grad do\n",
    "        text_features = model.encode_text(tokens)\n",
    "        text_features /= text_features.norm(dim: -1, keepdim: true) # Normalization is required!\n",
    "        text_features\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce99603-6d25-49ab-861c-4eed146e6eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display images\n",
    "def display_images(image_paths, distances)\n",
    "    image_paths.each_with_index do |path, idx|\n",
    "        IRuby.display(IRuby.image(path))\n",
    "        puts \"ðŸ‘† #{distances[idx]}\"\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc1a781-3a89-4e86-8232-ef8812b4a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will search and display nearest images given a text query\n",
    "nbrs = NearestNeighbors.new(n_neighbors: 10, metric: 'cosine')\n",
    "nbrs.fit(image_embeddings.cpu.numpy)\n",
    "\n",
    "def search(text)\n",
    "    text_embedding = embed_text(text)\n",
    "    distances, indices = nbrs.kneighbors(text_embedding.cpu.numpy)\n",
    "    nearest_images = indices[0].map { |i| image_paths[i] }\n",
    "    display_images(nearest_images, distances[0])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec92c08-f59d-4d70-b182-5eb717003619",
   "metadata": {},
   "outputs": [],
   "source": [
    "search('large kitchen island colonial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35396557-103f-4012-8a98-63e6ac0375e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "search('white marble shower stall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c734cd0-4299-4852-b6cf-0e125c21d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "search('red ferrari')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0efd47-95bd-4ac8-9727-4695d6658f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search('nuclear reactor')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 3.1.3",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "3.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
