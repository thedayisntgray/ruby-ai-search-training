{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84a25145-4603-48e6-94a9-0b1539d3d990",
   "metadata": {},
   "source": [
    "# Lab 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34b10926-169b-4172-9c7e-cb5eee4a6790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install in Docker\n",
    "# apt-get update\n",
    "# apt-get install -y libmagickwand-dev\n",
    "# gem install rmagick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0eaaaf1-dccf-44fe-a681-9b49588cba16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require 'httparty'\n",
    "require 'zip'\n",
    "# I dont think i need this:\n",
    "# require 'mini_magick'\n",
    "require 'torch-rb'\n",
    "# require 'open_clip'\n",
    "# require 'nearest_neighbors'\n",
    "require 'tqdm'\n",
    "require 'transformers-rb'\n",
    "require \"onnxruntime\"\n",
    "require 'rmagick'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e1fa3d-cbaa-4d94-bda5-4d287dc46809",
   "metadata": {},
   "source": [
    "## Get the image dataset (interiors of houses)\n",
    "\n",
    "- Source: https://www.kaggle.com/datasets/mikhailma/house-rooms-streets-image-dataset/data\n",
    "- Cached: https://max.io/house_data_png.zip (resized to 256x256 and converted to PNG)\n",
    "- License: CC-0 Public Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "328ec361-c278-4cb5-9c82-878c92668260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting https://max.io/house_data_png.zip\n"
     ]
    },
    {
     "ename": "Zip::DestinationFileExistsError",
     "evalue": "Destination './house_data_png/kitchen_587.png' already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[31mZip::DestinationFileExistsError\u001b[0m: Destination './house_data_png/kitchen_587.png' already exists",
      "/home/jovyan/.local/share/gem/ruby/3.1.0/gems/rubyzip-2.3.2/lib/zip/entry.rb:617:in `create_file'",
      "/home/jovyan/.local/share/gem/ruby/3.1.0/gems/rubyzip-2.3.2/lib/zip/entry.rb:187:in `extract'",
      "(irb):6:in `block (2 levels) in download_and_extract_zip'",
      "/home/jovyan/.local/share/gem/ruby/3.1.0/gems/rubyzip-2.3.2/lib/zip/entry_set.rb:38:in `block in each'",
      "/home/jovyan/.local/share/gem/ruby/3.1.0/gems/rubyzip-2.3.2/lib/zip/entry_set.rb:37:in `each'",
      "/home/jovyan/.local/share/gem/ruby/3.1.0/gems/rubyzip-2.3.2/lib/zip/entry_set.rb:37:in `each'",
      "/home/jovyan/.local/share/gem/ruby/3.1.0/gems/rubyzip-2.3.2/lib/zip/central_directory.rb:185:in `each'",
      "(irb):5:in `block in download_and_extract_zip'",
      "/home/jovyan/.local/share/gem/ruby/3.1.0/gems/rubyzip-2.3.2/lib/zip/file.rb:156:in `open_buffer'",
      "(irb):4:in `download_and_extract_zip'",
      "(irb):13:in `<top (required)>'"
     ]
    }
   ],
   "source": [
    "# Function to download and extract the zip file\n",
    "def download_and_extract_zip(url, extract_to: '.')\n",
    "    puts \"Downloading and extracting #{url}\"\n",
    "    response = HTTParty.get(url)\n",
    "    Zip::File.open_buffer(response.body) do |zip_file|\n",
    "        zip_file.each do |entry|\n",
    "            entry.extract(File.join(extract_to, entry.name))\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "# Download and extract the example images\n",
    "url = \"https://max.io/house_data_png.zip\"\n",
    "download_and_extract_zip(url)\n",
    "image_dir = 'house_data_png'\n",
    "image_paths = Dir[File.join(image_dir, '*')]\n",
    "# puts image_paths\n",
    "puts \"Extracted #{image_paths.length} images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ea09f4a-285a-4a38-8973-cb385494b851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "puts image_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13414644-6c22-4d11-b3ee-4468d191c4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6185f03-fc2c-4aec-ba99-eb78d43f8d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load our model\n",
    "# model, transform, preprocess = OpenClip.create_model_and_transforms('ViT-B-32', pretrained: 'openai')\n",
    "\n",
    "# # Print the model architecture\n",
    "# model.eval\n",
    "# puts model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5b03437-1fdf-4497-bc98-51b0b4dfb49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#<OnnxRuntime::Model:0x00007ffff127ff08 @session=#<OnnxRuntime::InferenceSession:0x00007ffff127fe18 @session=#<FFI::MemoryPointer address=0x000055555ec787a0 size=8>, @allocator=#<FFI::MemoryPointer address=0x000055555f2c0be0 size=8>, @inputs=[{:name=>\"IMAGE\", :type=>\"tensor(float)\", :shape=>[\"image_batch_size\", 3, 224, 224]}, {:name=>\"TEXT\", :type=>\"tensor(int32)\", :shape=>[\"text_batch_size\", 77]}], @outputs=[{:name=>\"LOGITS_PER_IMAGE\", :type=>\"tensor(float)\", :shape=>[\"image_batch_size\", \"text_batch_size\"]}, {:name=>\"LOGITS_PER_TEXT\", :type=>\"tensor(float)\", :shape=>[\"text_batch_size\", \"image_batch_size\"]}]>>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From notebook 07c\n",
    "model = OnnxRuntime::Model.new(\"clip_vit.onnx\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25cca691-9d0f-47a0-96f3-fce9c5aef3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{:name=>\"IMAGE\", :type=>\"tensor(float)\", :shape=>[\"image_batch_size\", 3, 224, 224]}, {:name=>\"TEXT\", :type=>\"tensor(int32)\", :shape=>[\"text_batch_size\", 77]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfd97065-d475-4f2b-9c1e-3cb60c6a3b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{:name=>\"LOGITS_PER_IMAGE\", :type=>\"tensor(float)\", :shape=>[\"image_batch_size\", \"text_batch_size\"]}, {:name=>\"LOGITS_PER_TEXT\", :type=>\"tensor(float)\", :shape=>[\"text_batch_size\", \"image_batch_size\"]}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c656b9e9-b0a0-435f-a074-60a7eb65c3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{:custom_metadata_map=>{}, :description=>\"\", :domain=>\"\", :graph_name=>\"main_graph\", :graph_description=>\"\", :producer_name=>\"pytorch\", :version=>9223372036854775807}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "002c5044-fd0b-412d-8be8-45c4018ccc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Numo::SFloat#shape=[5]\n",
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Torch.ones(5)\n",
    "a.numo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba939358-fc0d-42fc-b329-038a1005b12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], dtype: :int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = Numo::NArray.cast([1, 2, 3])\n",
    "Torch.from_numo(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f86c3df-6a9b-4313-8fd6-202a141aeea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image paths: nil\n",
      "Found 5249 images\n"
     ]
    }
   ],
   "source": [
    "# Check if paths are loaded\n",
    "puts \"Image paths: #{image_paths.inspect}\"\n",
    "\n",
    "# Verify directory path\n",
    "image_dir = 'house_data_png'\n",
    "image_paths = Dir[File.join(image_dir, '*')]\n",
    "puts \"Found #{image_paths.length} images\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19c475af-f2a2-494e-8c4b-edf57a428d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5249"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_paths.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43f3ce34-d6a7-47b8-bebb-b62ec1529ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"house_data_png/bath_100.png\", \"house_data_png/bath_1001.png\", \"house_data_png/bath_1003.png\", \"house_data_png/bath_1004.png\", \"house_data_png/bath_1005.png\", \"house_data_png/bath_1006.png\", \"house_data_png/bath_1007.png\", \"house_data_png/bath_1010.png\", \"house_data_png/bath_1011.png\", \"house_data_png/bath_1012.png\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_five_image_paths = image_paths.first(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ec4c6c0-a144-4a32-821f-1a89c88da740",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#happy path\n",
    "# def magick_image_to_tensor(image)\n",
    "#   # 1) Resize to 224×224\n",
    "#   image = image.resize_to_fill(224, 224)\n",
    "\n",
    "#   # 2) Convert to RGB\n",
    "#   image = image.quantize(256, Magick::RGBColorspace)\n",
    "\n",
    "#   # 3) Export pixel data in \"RGB\" order ([0..65535])\n",
    "#   pixels = image.export_pixels(0, 0, image.columns, image.rows, \"RGB\")\n",
    "\n",
    "#   # 4) Convert to float32 and scale to [0,1]\n",
    "#   arr = Numo::UInt16.asarray(pixels).cast_to(Numo::Float32) / 65535.0\n",
    "\n",
    "#   # 5) Reshape from [height, width, channels] -> [channels, height, width]\n",
    "#   arr = arr.reshape(image.rows, image.columns, 3)\n",
    "#   arr = arr.transpose(2, 0, 1)\n",
    "\n",
    "#   # 6) Convert to a Torch tensor and add a batch dimension => [1, 3, 224, 224]\n",
    "#   Torch.from_numo(arr).unsqueeze(0)\n",
    "# end\n",
    "\n",
    "# def get_image_embeddings(model, image_paths, batch_size: 32)\n",
    "#   embeddings_list = []\n",
    "\n",
    "#   (0...image_paths.size).step(batch_size).each do |start_idx|\n",
    "#     # Get the image subset for this batch\n",
    "#     batch_paths = image_paths[start_idx, batch_size]\n",
    "\n",
    "#     # Convert each image to Torch tensor => shape [1,3,224,224]\n",
    "#     batch_tensors = batch_paths.map do |path|\n",
    "#       img_tensor = magick_image_to_tensor(Magick::Image.read(path).first)\n",
    "#       img_tensor\n",
    "#     end\n",
    "\n",
    "#     # Concatenate => shape [B,3,224,224]\n",
    "#     batch_tensor = Torch.cat(batch_tensors, dim: 0)\n",
    "\n",
    "#     # Convert Torch tensor -> Numo::NArray ([B,3,224,224]) for ONNXRuntime\n",
    "#     image_data = batch_tensor.numo  # or Torch::Utils.to_numo(batch_tensor)\n",
    "\n",
    "#     # ---------------------------------------------------------------\n",
    "#     # Provide a dummy text_input so the model doesn't complain.\n",
    "#     # For example, if the model expects [B,77] integer IDs for text.\n",
    "#     # (Adjust shape and type as your model needs; this is just a placeholder.)\n",
    "#     batch_size_actual = batch_tensor.shape[0]   # or batch_paths.size\n",
    "#     dummy_text = Numo::Int32.zeros(batch_size_actual, 77)\n",
    "#     # ---------------------------------------------------------------\n",
    "\n",
    "#     # Predict with image_input + text_input\n",
    "#     outputs = model.predict({\n",
    "#       \"image_input\" => image_data,\n",
    "#       \"text_input\"  => dummy_text\n",
    "#     })\n",
    "\n",
    "#     # If \"logits_per_image\" is your desired embedding output\n",
    "#     batch_embeddings = outputs[\"logits_per_image\"]\n",
    "#       puts batch_embeddings.class\n",
    "\n",
    "#     # Convert embeddings back to Torch (optional)\n",
    "#     b = Numo::NArray.cast(batch_embeddings)\n",
    "#     # Torch.from_numo(b)\n",
    "#     batch_embeddings_torch = Torch.from_numo(b)\n",
    "\n",
    "#     embeddings_list << batch_embeddings_torch\n",
    "#   end\n",
    "\n",
    "#   # Concatenate everything into a single [N, ...] tensor\n",
    "#   Torch.cat(embeddings_list, dim: 0)\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba7d58f7-a9e1-439e-9f4d-eaf3f872f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# require 'rmagick'\n",
    "\n",
    "\n",
    "# def magick_image_to_tensor(image)\n",
    "#   image = image.resize_to_fill(224, 224)\n",
    "#   image = image.quantize(256, Magick::RGBColorspace)\n",
    "#   pixels = image.export_pixels(0, 0, image.columns, image.rows, \"RGB\")\n",
    "#   arr = Numo::UInt16.asarray(pixels).cast_to(Numo::Float32) / 65535.0\n",
    "#   arr = arr.reshape(image.rows, image.columns, 3)\n",
    "#   arr = arr.transpose(2, 0, 1)\n",
    "#   # Convert to tensor and normalize using CLIP's expected mean/std\n",
    "#   tensor = Torch.from_numo(arr)\n",
    "#   mean = Torch.tensor([0.48145466, 0.4578275, 0.40821073]).reshape(3, 1, 1)\n",
    "#   std = Torch.tensor([0.26862954, 0.26130258, 0.27577711]).reshape(3, 1, 1)\n",
    "#   ((tensor - mean) / std).unsqueeze(0)\n",
    "    \n",
    "#       # Add debug prints\n",
    "#   puts \"Array shape after reshape: #{arr.shape}\"\n",
    "#   puts \"First few values before normalization: #{arr[0, 0, 0..5]}\"\n",
    "  \n",
    "#   tensor = Torch.from_numo(arr)\n",
    "#   mean = Torch.tensor([0.48145466, 0.4578275, 0.40821073]).reshape(3, 1, 1)\n",
    "#   std = Torch.tensor([0.26862954, 0.26130258, 0.27577711]).reshape(3, 1, 1)\n",
    "#   normalized = ((tensor - mean) / std).unsqueeze(0)\n",
    "  \n",
    "#   puts \"First few values after normalization: #{normalized[0, 0, 0, 0..5]}\"\n",
    "#   normalized\n",
    "# end\n",
    "\n",
    "\n",
    "\n",
    "# def get_image_embeddings(model, image_paths, batch_size: 32)\n",
    "#   embeddings_list = []\n",
    "  \n",
    "#   (0...image_paths.size).step(batch_size).each do |start_idx|\n",
    "#     batch_paths = image_paths[start_idx, batch_size]\n",
    "#     batch_tensors = batch_paths.map do |path|\n",
    "#       img_tensor = magick_image_to_tensor(Magick::Image.read(path).first)\n",
    "#       img_tensor\n",
    "#     end\n",
    "    \n",
    "#     batch_tensor = Torch.cat(batch_tensors, dim: 0)\n",
    "#     image_data = batch_tensor.numo\n",
    "    \n",
    "#     outputs = model.predict({\n",
    "#       \"image_input\" => image_data,\n",
    "#       \"text_input\" => Numo::Int32.zeros(batch_tensor.shape[0], 77)\n",
    "#     })\n",
    "    \n",
    "#     # Get image embeddings directly instead of logits\n",
    "#     batch_embeddings = outputs[\"image_features\"]  # Try image_features first\n",
    "#     if batch_embeddings.nil?\n",
    "#       # If no image_features, try alternate keys\n",
    "#       puts \"Available keys: #{outputs.keys}\"\n",
    "#       batch_embeddings = outputs[\"image_embeddings\"] || \n",
    "#                         outputs[\"visual_features\"] ||\n",
    "#                         outputs[\"visual_embeddings\"]\n",
    "#     end\n",
    "    \n",
    "#     batch_embeddings_arr = Numo::NArray.cast(batch_embeddings)\n",
    "#     batch_embeddings_torch = Torch.from_numo(batch_embeddings_arr)\n",
    "#     embeddings_list << batch_embeddings_torch\n",
    "#   end\n",
    "  \n",
    "#   combined_embeddings = Torch.cat(embeddings_list, dim: 0)\n",
    "  \n",
    "#   # L2 normalize the embeddings\n",
    "#   norms = combined_embeddings.norm(dim: -1, keepdim: true)\n",
    "#   combined_embeddings / norms\n",
    "# end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a8306e3-67d0-4504-9f8d-7bdca4df24ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def magick_image_to_tensor(image)\n",
    "#   image = image.resize_to_fill(224, 224)\n",
    "#   image = image.quantize(256, Magick::RGBColorspace)\n",
    "#   pixels = image.export_pixels(0, 0, image.columns, image.rows, \"RGB\")\n",
    "#   arr = Numo::UInt16.asarray(pixels).cast_to(Numo::Float32) / 65535.0\n",
    "#   arr = arr.reshape(image.rows, image.columns, 3)\n",
    "#   arr = arr.transpose(2, 0, 1)\n",
    "#   tensor = Torch.from_numo(arr)\n",
    "#   mean = Torch.tensor([0.48145466, 0.4578275, 0.40821073]).reshape(3, 1, 1)\n",
    "#   std = Torch.tensor([0.26862954, 0.26130258, 0.27577711]).reshape(3, 1, 1)\n",
    "#   ((tensor - mean) / std).unsqueeze(0)\n",
    "# end\n",
    "\n",
    "# def get_image_embeddings(model, image_paths, batch_size: 32)\n",
    "#   embeddings_list = []\n",
    "  \n",
    "#   (0...image_paths.size).step(batch_size).each do |start_idx|\n",
    "#     batch_paths = image_paths[start_idx, batch_size]\n",
    "#     batch_tensors = batch_paths.map do |path|\n",
    "#       img_tensor = magick_image_to_tensor(Magick::Image.read(path).first)\n",
    "#       img_tensor\n",
    "#     end\n",
    "    \n",
    "#     batch_tensor = Torch.cat(batch_tensors, dim: 0)\n",
    "#     image_data = batch_tensor.numo\n",
    "    \n",
    "#     # Only pass image_input since we exported only the image encoder\n",
    "#     outputs = model.predict({\n",
    "#       \"image_input\" => image_data\n",
    "#     })\n",
    "    \n",
    "#     # Get image features directly\n",
    "#     batch_embeddings = outputs[\"image_features\"]\n",
    "#     batch_embeddings_arr = Numo::NArray.cast(batch_embeddings)\n",
    "#     batch_embeddings_torch = Torch.from_numo(batch_embeddings_arr)\n",
    "#     embeddings_list << batch_embeddings_torch\n",
    "#   end\n",
    "  \n",
    "#   combined_embeddings = Torch.cat(embeddings_list, dim: 0)\n",
    "  \n",
    "#   # L2 normalize the embeddings\n",
    "#   norms = combined_embeddings.norm(dim: -1, keepdim: true)\n",
    "#   combined_embeddings / norms\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd9bcc7c-1a1e-4754-bb0c-9e9b88900590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":magick_image_to_tensor"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def magick_image_to_tensor(image)\n",
    "  image = image.resize_to_fill(224, 224)\n",
    "  image = image.quantize(256, Magick::RGBColorspace)\n",
    "  pixels = image.export_pixels(0, 0, image.columns, image.rows, \"RGB\")\n",
    "  arr = Numo::UInt16.asarray(pixels).cast_to(Numo::Float32) / 65535.0\n",
    "  arr = arr.reshape(image.rows, image.columns, 3)\n",
    "  arr = arr.transpose(2, 0, 1)\n",
    "  # Convert to tensor and normalize using CLIP's expected mean/std\n",
    "  tensor = Torch.from_numo(arr)\n",
    "  mean = Torch.tensor([0.48145466, 0.4578275, 0.40821073]).reshape(3, 1, 1)\n",
    "  std = Torch.tensor([0.26862954, 0.26130258, 0.27577711]).reshape(3, 1, 1)\n",
    "  ((tensor - mean) / std).unsqueeze(0)\n",
    "    \n",
    "      # Add debug prints\n",
    "  puts \"Array shape after reshape: #{arr.shape}\"\n",
    "  puts \"First few values before normalization: #{arr[0, 0, 0..5]}\"\n",
    "  \n",
    "  tensor = Torch.from_numo(arr)\n",
    "  mean = Torch.tensor([0.48145466, 0.4578275, 0.40821073]).reshape(3, 1, 1)\n",
    "  std = Torch.tensor([0.26862954, 0.26130258, 0.27577711]).reshape(3, 1, 1)\n",
    "  normalized = ((tensor - mean) / std).unsqueeze(0)\n",
    "  \n",
    "  puts \"First few values after normalization: #{normalized[0, 0, 0, 0..5]}\"\n",
    "  normalized\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "# def get_image_embeddings(model, image_paths, batch_size: 32)\n",
    "#   embeddings_list = []\n",
    "  \n",
    "#   (0...image_paths.size).step(batch_size).each do |start_idx|\n",
    "#     batch_paths = image_paths[start_idx, batch_size]\n",
    "#     batch_tensors = batch_paths.map do |path|\n",
    "#       img_tensor = magick_image_to_tensor(Magick::Image.read(path).first)\n",
    "#       img_tensor\n",
    "#     end\n",
    "    \n",
    "#     batch_tensor = Torch.cat(batch_tensors, dim: 0)\n",
    "#     image_data = batch_tensor.numo\n",
    "    \n",
    "#     # If your ONNX has *only* \"image_input\":\n",
    "#     outputs = model.predict({ \"image_input\" => image_data })\n",
    "\n",
    "#     # Otherwise, if you had a two-input model, remove or adjust text_input:\n",
    "#     # outputs = model.predict({\n",
    "#     #   \"image_input\" => image_data,\n",
    "#     #   \"text_input\"  => Numo::Int32.zeros(batch_tensor.shape[0], 77)\n",
    "#     # })\n",
    "\n",
    "#     # Grab the output embeddings\n",
    "#     batch_embeddings = outputs[\"image_features\"] \n",
    "#     # If nil, try alternate keys\n",
    "#     if batch_embeddings.nil?\n",
    "#       puts \"Available keys: #{outputs.keys}\"\n",
    "#       batch_embeddings = outputs[\"image_embeddings\"] || \n",
    "#                         outputs[\"visual_features\"] ||\n",
    "#                         outputs[\"visual_embeddings\"]\n",
    "#     end\n",
    "    \n",
    "#     batch_embeddings_arr = Numo::NArray.cast(batch_embeddings)\n",
    "#     batch_embeddings_torch = Torch.from_numo(batch_embeddings_arr)\n",
    "#     embeddings_list << batch_embeddings_torch\n",
    "#   end\n",
    "  \n",
    "#   combined_embeddings = Torch.cat(embeddings_list, dim: 0)\n",
    "#   puts \"Combined shape: #{combined_embeddings.shape}\"\n",
    "\n",
    "#   # L2 normalize across the last dimension (dim=1 for [batch_size, 512])\n",
    "#   norms = combined_embeddings.norm(2, 1, keepdim: true)\n",
    "#   combined_embeddings / norms\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0bd5eedd-06a7-4e05-9f62-09dab2a0ab8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":get_image_embeddings"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_image_embeddings(model, image_paths, batch_size: 32)\n",
    "  embeddings_list = []\n",
    "  \n",
    "  (0...image_paths.size).step(batch_size).each do |start_idx|\n",
    "    batch_paths = image_paths[start_idx, batch_size]\n",
    "    batch_tensors = batch_paths.map do |path|\n",
    "      img_tensor = magick_image_to_tensor(Magick::Image.read(path).first)\n",
    "      img_tensor\n",
    "    end\n",
    "    \n",
    "    batch_tensor = Torch.cat(batch_tensors, dim: 0)\n",
    "    image_data = batch_tensor.numo\n",
    "    \n",
    "    # Create dummy text inputs. Assuming text tokens are integers and the length is 77\n",
    "    dummy_texts = Numo::Int32.zeros(batch_tensor.shape[0], 77)\n",
    "    \n",
    "    # Provide both IMAGE and TEXT inputs\n",
    "    outputs = model.predict({\n",
    "      \"IMAGE\" => image_data,\n",
    "      \"TEXT\"  => dummy_texts\n",
    "    })\n",
    "\n",
    "    # Grab the output embeddings\n",
    "    batch_embeddings = outputs[\"LOGITS_PER_IMAGE\"] \n",
    "    # Adjust the key based on your model's output\n",
    "    # If nil, try alternate keys\n",
    "    if batch_embeddings.nil?\n",
    "      puts \"Available keys: #{outputs.keys}\"\n",
    "      batch_embeddings = outputs[\"image_embeddings\"] || \n",
    "                        outputs[\"image_embeddings\"] ||\n",
    "                        outputs[\"visual_features\"] ||\n",
    "                        outputs[\"visual_embeddings\"]\n",
    "    end\n",
    "    \n",
    "    batch_embeddings_arr = Numo::NArray.cast(batch_embeddings)\n",
    "    batch_embeddings_torch = Torch.from_numo(batch_embeddings_arr)\n",
    "    embeddings_list << batch_embeddings_torch\n",
    "  end\n",
    "  \n",
    "  combined_embeddings = Torch.cat(embeddings_list, dim: 0)\n",
    "  puts \"Combined shape: #{combined_embeddings.shape}\"\n",
    "\n",
    "  # L2 normalize across the last dimension (dim=1 for [batch_size, 512])\n",
    "  norms = combined_embeddings.norm(2, 1, keepdim: true)\n",
    "  combined_embeddings / norms\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c976b01d-6b26-4aff-90e3-4685a010247a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array shape after reshape: [3, 224, 224]\n",
      "First few values before normalization: #<Numo::SFloat:0x00007ffff01ed9d8>\n",
      "First few values after normalization: tensor([-0.6897, -0.8876, -0.7553, -0.6617, -0.6897, -0.6897])\n",
      "Array shape after reshape: [3, 224, 224]\n",
      "First few values before normalization: #<Numo::SFloat:0x00007ffff01f8b58>\n",
      "First few values after normalization: tensor([0.9548, 0.7210, 1.4227, 1.5530, 1.7433, 1.8676])\n",
      "Array shape after reshape: [3, 224, 224]\n",
      "First few values before normalization: #<Numo::SFloat:0x00007ffff01fe238>\n",
      "First few values after normalization: tensor([1.9291, 1.9291, 1.9291, 1.9291, 1.9291, 1.9291])\n",
      "Array shape after reshape: [3, 224, 224]\n",
      "First few values before normalization: #<Numo::SFloat:0x00007ffff02003d0>\n",
      "First few values after normalization: tensor([1.9259, 1.9259, 1.9259, 1.9259, 1.9259, 1.9259])\n",
      "Array shape after reshape: [3, 224, 224]\n",
      "First few values before normalization: #<Numo::SFloat:0x00007ffffb5113c0>\n",
      "First few values after normalization: tensor([-1.2776, -1.3015, -1.2776, -1.2773, -1.1603, -1.2773])\n",
      "Array shape after reshape: [3, 224, 224]\n",
      "First few values before normalization: #<Numo::SFloat:0x00007ffffb51c720>\n",
      "First few values after normalization: tensor([1.9210, 1.9210, 1.9210, 1.9210, 1.9210, 1.9210])\n",
      "Array shape after reshape: [3, 224, 224]\n",
      "First few values before normalization: #<Numo::SFloat:0x00007ffffb55c0a0>\n",
      "First few values after normalization: tensor([1.1196, 1.1196, 1.0551, 1.2877, 1.2877, 1.3316])\n",
      "Array shape after reshape: [3, 224, 224]\n",
      "First few values before normalization: #<Numo::SFloat:0x00007ffffb5e0a08>\n",
      "First few values after normalization: tensor([1.9230, 1.9230, 1.9230, 1.9230, 1.9230, 1.9230])\n",
      "Array shape after reshape: [3, 224, 224]\n",
      "First few values before normalization: #<Numo::SFloat:0x00007ffffaf88d68>\n",
      "First few values after normalization: tensor([-1.6291, -1.3939, -1.2376, -1.1585, -1.0689, -1.2376])\n",
      "Array shape after reshape: [3, 224, 224]\n",
      "First few values before normalization: #<Numo::SFloat:0x00007ffffafa7808>\n",
      "First few values after normalization: tensor([1.9287, 1.9287, 1.9287, 1.9287, 1.9287, 1.9287])\n",
      "Combined shape: [10, 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162,\n",
       "         0.3162],\n",
       "        [0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162,\n",
       "         0.3162],\n",
       "        [0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162,\n",
       "         0.3162],\n",
       "        [0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162,\n",
       "         0.3162],\n",
       "        [0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162,\n",
       "         0.3162],\n",
       "        [0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162,\n",
       "         0.3162],\n",
       "        [0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162,\n",
       "         0.3162],\n",
       "        [0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162,\n",
       "         0.3162],\n",
       "        [0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162,\n",
       "         0.3162],\n",
       "        [0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162, 0.3162,\n",
       "         0.3162]], dtype: :float64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then call the function\n",
    "# image_embeddings = get_image_embeddings(image_paths, batch_size: 32)\n",
    "\n",
    "#Lets do the first 5 for now\n",
    "image_embeddings = get_image_embeddings(model, first_five_image_paths, batch_size: 32)\n",
    "image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec192bc5-1752-4abd-b436-de04b113859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# norms = Torch.norm(image_embeddings, p: 2.0, dim: 1, keepdim: true)\n",
    "# image_embeddings = image_embeddings / norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "227a24e4-eeeb-4dd2-9aef-92a5f2789e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "puts 'hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8278584d-363e-4d30-83c2-4bfa0da5a8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch::Tensor\n"
     ]
    }
   ],
   "source": [
    "# puts image_embeddings\n",
    "puts image_embeddings.class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6b2d7ce-a18c-40c7-9dcb-37e17728ef8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected a Numo::NArray, got Torch::Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[31mRuntimeError\u001b[0m: Expected a Numo::NArray, got Torch::Tensor",
      "(irb):21:in `normalize'",
      "(irb):35:in `<top (required)>'"
     ]
    }
   ],
   "source": [
    "require 'numo/narray'\n",
    "\n",
    "def normalize(embeddings)\n",
    "  # 1. If 'embeddings' is actually an Array of Numo arrays, handle that:\n",
    "  if embeddings.is_a?(Array) && !embeddings.empty?\n",
    "    # If it's exactly one Numo array, unwrap it:\n",
    "    if embeddings.size == 1 && embeddings.first.is_a?(Numo::NArray)\n",
    "      embeddings = embeddings.first\n",
    "    else\n",
    "      # If it's multiple Numo arrays, concatenate them\n",
    "      # (assuming they have the same shape except for the first dimension)\n",
    "      if embeddings.all? { |x| x.is_a?(Numo::NArray) }\n",
    "        embeddings = Numo::NArray.concatenate(embeddings, axis: 0)\n",
    "      else\n",
    "        raise \"Expected an array of Numo::NArray. Got: #{embeddings.inspect}\"\n",
    "      end\n",
    "    end\n",
    "  end\n",
    "\n",
    "  # 2. Now 'embeddings' should be a single Numo::NArray.  \n",
    "  unless embeddings.is_a?(Numo::NArray)\n",
    "    raise \"Expected a Numo::NArray, got #{embeddings.class}\"\n",
    "  end\n",
    "\n",
    "  # 3. Convert Numo array -> Torch tensor\n",
    "  tensor = Torch.from_numo(embeddings)\n",
    "\n",
    "  # 4. Compute L2 norm over the last dimension => shape is kept\n",
    "  norm = (tensor * tensor).sum(-1, keepdim: true).sqrt\n",
    "\n",
    "  # 5. Divide by norm, convert back to Numo\n",
    "  (tensor / norm).numo\n",
    "end\n",
    "\n",
    "# Example usage:\n",
    "image_embeddings = normalize(image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "56f2b810-67d1-494d-a0b3-31a75fc6983f",
   "metadata": {},
   "outputs": [
    {
     "ename": "Torch::Error",
     "evalue": "Cannot convert Torch::Tensor to tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[31mTorch::Error\u001b[0m: Cannot convert Torch::Tensor to tensor",
      "/home/jovyan/.local/share/gem/ruby/3.1.0/gems/torch-rb-0.18.0/lib/torch.rb:331:in `from_numo'",
      "(irb):2:in `normalize'",
      "(irb):13:in `<top (required)>'"
     ]
    }
   ],
   "source": [
    "def normalize(embeddings)\n",
    "  # Convert Numo array -> Torch tensor\n",
    "  tensor = Torch.from_numo(embeddings)  # shape is the same as embeddings\n",
    "\n",
    "  # L2 norm over the last dimension\n",
    "  # .sum(-1, keepdim: true) => sums squares along the last dimension\n",
    "  norm = (tensor * tensor).sum(-1, keepdim: true).sqrt\n",
    "\n",
    "  # Elementwise divide, then convert back to Numo\n",
    "  (tensor / norm).numo\n",
    "end\n",
    "\n",
    "# Example usage\n",
    "image_embeddings = normalize(image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d13d9b05-1071-4ace-b932-342594844248",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "sum() missing 1 required positional arguments: \"dim\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31mArgumentError\u001b[0m: sum() missing 1 required positional arguments: \"dim\"",
      "(irb):5:in `sum'",
      "(irb):5:in `normalize_embeddings'",
      "(irb):17:in `<top (required)>'"
     ]
    }
   ],
   "source": [
    "def normalize_embeddings(embeddings)\n",
    "  # Square each element\n",
    "  squared = embeddings ** 2\n",
    "  \n",
    "  # Sum along the last dimension (keepdims: true maintains the dimension)\n",
    "  sum_squared = squared.sum(axis: -1, keepdims: true)\n",
    "  \n",
    "  # Calculate square root\n",
    "  norm = sum_squared ** 0.5\n",
    "  \n",
    "  # Divide by norm to normalize\n",
    "  normalized = embeddings / norm\n",
    "\n",
    "  normalized\n",
    "end\n",
    "\n",
    "# Use the function\n",
    "image_embeddings = normalize_embeddings(image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e23e59c9-ae46-41cb-aa96-133019d20502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalization is required!\n",
    "# image_embeddings /= image_embeddings.norm(dim: -1, keepdim: true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a31d3332-722d-4712-a852-7a7ab2ebe88e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "no _dump_data is defined for class Torch::Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[31mTypeError\u001b[0m: no _dump_data is defined for class Torch::Tensor",
      "(irb):1:in `dump'",
      "(irb):1:in `block in <top (required)>'",
      "(irb):in `open'",
      "(irb):in `<top (required)>'"
     ]
    }
   ],
   "source": [
    "File.open('house_data_png.marshal', 'wb') do |file|\n",
    "  Marshal.dump(image_embeddings, file)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "35dd0ce7-58ce-437b-ac85-b0b431e34154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save embeddings\n",
    "# File.open('house_data_png.marshal', 'wb') do |file|\n",
    "#     Marshal.dump(image_embeddings.cpu.numpy, file)\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "add83149-254f-4ce9-974c-fc0cd7caacf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 [10, 10]\n"
     ]
    }
   ],
   "source": [
    "puts \"#{image_embeddings.length} #{image_embeddings.shape}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "36083c88-f25a-4178-a932-b2d5804a469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# puts \"#{image_embeddings.length} #{image_embeddings[0].shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "70b35845-38ce-49fe-a285-cdd6f81ef81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":embed_text"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encodes the text to the same vector space as the images\n",
    "def embed_text(text)\n",
    "    tokens = OpenClip::Tokenizer.tokenize([text])\n",
    "    Torch.no_grad do\n",
    "        text_features = model.encode_text(tokens)\n",
    "        text_features /= text_features.norm(dim: -1, keepdim: true) # Normalization is required!\n",
    "        text_features\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cce99603-6d25-49ab-861c-4eed146e6eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":display_images"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to display images\n",
    "def display_images(image_paths, distances)\n",
    "    image_paths.each_with_index do |path, idx|\n",
    "        IRuby.display(IRuby.image(path))\n",
    "        puts \"👆 #{distances[idx]}\"\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bc1a781-3a89-4e86-8232-ef8812b4a777",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "uninitialized constant NearestNeighbors",
     "output_type": "error",
     "traceback": [
      "\u001b[31mNameError\u001b[0m: uninitialized constant NearestNeighbors",
      "(irb):1:in `<top (required)>'"
     ]
    }
   ],
   "source": [
    "# This will search and display nearest images given a text query\n",
    "nbrs = NearestNeighbors.new(n_neighbors: 10, metric: 'cosine')\n",
    "nbrs.fit(image_embeddings.cpu.numpy)\n",
    "\n",
    "def search(text)\n",
    "    text_embedding = embed_text(text)\n",
    "    distances, indices = nbrs.kneighbors(text_embedding.cpu.numpy)\n",
    "    nearest_images = indices[0].map { |i| image_paths[i] }\n",
    "    display_images(nearest_images, distances[0])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fec92c08-f59d-4d70-b182-5eb717003619",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoMethodError",
     "evalue": "undefined method `search' for #<Object:0x00007ffffaf83a98>",
     "output_type": "error",
     "traceback": [
      "\u001b[31mNoMethodError\u001b[0m: undefined method `search' for #<Object:0x00007ffffaf83a98>",
      "(irb):in `<top (required)>'"
     ]
    }
   ],
   "source": [
    "search('large kitchen island colonial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35396557-103f-4012-8a98-63e6ac0375e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoMethodError",
     "evalue": "undefined method `search' for #<Object:0x00007ffffaf83a98>",
     "output_type": "error",
     "traceback": [
      "\u001b[31mNoMethodError\u001b[0m: undefined method `search' for #<Object:0x00007ffffaf83a98>",
      "(irb):in `<top (required)>'"
     ]
    }
   ],
   "source": [
    "search('white marble shower stall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c734cd0-4299-4852-b6cf-0e125c21d913",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoMethodError",
     "evalue": "undefined method `search' for #<Object:0x00007ffffaf83a98>",
     "output_type": "error",
     "traceback": [
      "\u001b[31mNoMethodError\u001b[0m: undefined method `search' for #<Object:0x00007ffffaf83a98>",
      "(irb):in `<top (required)>'"
     ]
    }
   ],
   "source": [
    "search('red ferrari')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d0efd47-95bd-4ac8-9727-4695d6658f5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoMethodError",
     "evalue": "undefined method `search' for #<Object:0x00007ffffaf83a98>",
     "output_type": "error",
     "traceback": [
      "\u001b[31mNoMethodError\u001b[0m: undefined method `search' for #<Object:0x00007ffffaf83a98>",
      "(irb):in `<top (required)>'"
     ]
    }
   ],
   "source": [
    "search('nuclear reactor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04624e3-fd11-4eb3-a463-62525880a895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a770ae26-9a2e-4063-ba0b-6fda3c3c1290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2a3c01-b3aa-4052-b6d3-ddb737d59a36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b3f47-0eab-4117-85ef-7545565e9807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eaba15-1789-4330-b46f-a385866c3027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 3.1.3",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "3.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
